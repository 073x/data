{
  "copyright_text": null,
  "description": "Speaker: Tania Allard\n\nIn data science (in its all its variants) a significant part of an individual\u2019s time is spent preparing data into a digestible format. In general, a data science pipeline starts with the acquisition of raw data which is then manipulated through ETL processes and leads to a series of analytics.\nGood data pipelines can be used to automate and schedule these steps, help with monitoring tasks, and even to dynamically train models. On top of that, they make the analyses easier to reproduce and productise.\n\nIn this workshop, you will learn how to migrate from \u2018scripts soups\u2019 (a set of scripts that should be run in a particular order) to robust, reproducible and easy-to-schedule data pipelines in Airflow. First, we will learn how to write simple recurrent ETL pipelines. We will then integrate logging and monitoring capabilities. And we will end using Airflow along with Jupyter Notebooks and paper mill to produce reproducible analytics reports.\n\n\nSlides can be found at: https://speakerdeck.com/pycon2019 and https://github.com/PyCon/2019-slides",
  "duration": 11875,
  "language": "eng",
  "recorded": "2019-05-02",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://us.pycon.org/2019/schedule/talks/"
    },
    {
      "label": "https://speakerdeck.com/pycon2019",
      "url": "https://speakerdeck.com/pycon2019"
    },
    {
      "label": "https://github.com/PyCon/2019-slides",
      "url": "https://github.com/PyCon/2019-slides"
    }
  ],
  "speakers": [
    "TODO"
  ],
  "tags": [
    "Tania Allard",
    "coding",
    "pycon",
    "python",
    "tutorial"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/n9_JjmHRtys/hqdefault.jpg",
  "title": "Tania Allard - Building data pipelines in Python: Airflow vs scripts soup - PyCon 2019",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=n9_JjmHRtys"
    }
  ]
}
