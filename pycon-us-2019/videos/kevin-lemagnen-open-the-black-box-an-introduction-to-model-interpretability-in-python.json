{
  "copyright_text": null,
  "description": "Speaker: Kevin Lemagnen\n\nWhat's the use of sophisticated machine learning models if you can't interpret them?\n\nIn fact, many industries including finance and healthcare require clear explanations of why a decision is made. This tutorial covers recent model interpretability techniques that are essentials in your data scientist toolbox: Eli5, LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive exPlanations).\n\nYou will learn how to apply these techniques in Python on real-world data science problems in order to debug your models and explain their decisions.\n\nYou will also learn the conceptual background behind these techniques so you can better understand when they are appropriate.\n\nSlides can be found at: https://speakerdeck.com/pycon2019 and https://github.com/PyCon/2019-slides",
  "duration": 13102,
  "language": "eng",
  "recorded": "2019-05-04",
  "related_urls": [
    {
      "label": "Conference schedule",
      "url": "https://us.pycon.org/2019/schedule/talks/"
    },
    {
      "label": "https://speakerdeck.com/pycon2019",
      "url": "https://speakerdeck.com/pycon2019"
    },
    {
      "label": "https://github.com/PyCon/2019-slides",
      "url": "https://github.com/PyCon/2019-slides"
    }
  ],
  "speakers": [
    "TODO"
  ],
  "tags": [
    "Kevin Lemagnen",
    "coding",
    "pycon",
    "python",
    "tutorial"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/d66ttkMY9zE/maxresdefault.jpg",
  "title": "Kevin Lemagnen - Open the Black Box: an Introduction to Model Interpretability in Python",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=d66ttkMY9zE"
    }
  ]
}
