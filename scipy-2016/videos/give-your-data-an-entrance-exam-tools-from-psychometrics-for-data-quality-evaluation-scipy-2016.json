{
  "copyright_text": "Standard YouTube License",
  "description": "We discuss a means by which item response theory (IRT), originally developed as a psychometric tool for assessing a\nperson's intellectual or academic ability given their\nperformance on a standardized test, can be used as a\ndata quality tool. Assuming that a dataset has an\nunderlying \"ability\" to train predictive models (where\nthe ability is specific to the type of dependent variable\nbeing predicted), we build many models on top\nof a variety of datasets to simultaneously assess the\nbest dataset for a given dependent variable as well\nas which cases are the most \"difficult\" for a dataset\nto predict correctly. The product of this work is an\nunderstanding of both which predictions are the \"hardest\"\nto get correct for any dataset, as well as which dataset\nis expected to give the best predictions on a new\ndependent variable.\n\nThe first step in this study is to build a laboratory in which many related models can be trained and validated, reproducibly and\nin a self-documenting way. By running many models that\nlook at related dependent variables, for example, a number\nof variables meant to predict different aspects of political\nbehavior, we can characterize a baseline expected performance\nfor any new model similar to those already built.\nWe call this suite of related models a market basket, after the\nterminology and methodology used by economists to summarize\nthe state of a market.\n\nThen, when we investigate new data sources or formats, we\nhave a well-defined process for determining whether the\nnew data makes the models better--we re-build our market\nbasket, and compare the results with the new data to the\nresults without it (performance, model build time,\ndata storage constraints) to assess the quality of our\ndata in a way that is driven by the models and data itself.\n\nAn interesting question is how to assess whether a given\ndataset or feature is \"better\" for a given basket of models. An interesting idea comes to us from the field of psychometrics, which uses a set of tools called item response theory to assess exams (such as the SAT and GRE) and use exams to rank students by intellectual or academic ability.\n\nBorrowing the terminology of IRT, we draw the analogy that a dataset is like a student (it has an inherent capability to accomplish certain tasks, like building good models), a single model prediction is a test question, and full set of test predictions is an exam. IRT parameterizes both the (unknown) student ability and the (also unknown) test question difficulty, and uses the EM algorithm to simultaneously solve for the parameters of both quantities at once. This allows a researcher to know both how \"smart\" a dataset is for solving a given basket of models, as well as rank-order \"exam questions\" (model predictions) by difficulty. The result is a single methodology with applications for both data quality and assessing the difficulty of making a given prediction (useful for e.g. outlier identification).",
  "duration": 1804,
  "id": 5328,
  "language": "eng",
  "recorded": "2016-07-14",
  "related_urls": [],
  "slug": "give-your-data-an-entrance-exam-tools-from-psychometrics-for-data-quality-evaluation-scipy-2016",
  "speakers": [
    "Katie Malone"
  ],
  "tags": [
    "data science"
  ],
  "thumbnail_url": "https://i.ytimg.com/vi/zHMtzpc67zA/maxresdefault.jpg",
  "title": "Give Your Data an Entrance Exam: Tools from Psychometrics for Data Quality Evaluation",
  "videos": [
    {
      "type": "youtube",
      "url": "https://www.youtube.com/watch?v=zHMtzpc67zA"
    }
  ]
}
